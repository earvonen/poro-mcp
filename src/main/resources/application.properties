# Quarkus MCP Server Configuration

# OpenAI/vLLM API Configuration
# These values are read from environment variables with fallback defaults
# REST client base URL (required for Quarkus REST client)
%prod.openai/mp-rest/url=${VLLM_BASE_URL:http://vllm:8000}
%prod.openai.base-url=${VLLM_BASE_URL:http://vllm:8000}
%prod.openai.model-name=${PORO2_MODEL_NAME:LumiOpen/Llama-Poro-2-70B-Instruct}
%prod.openai.api-key=${VLLM_API_KEY:}
%prod.openai.default-temperature=${DEFAULT_TEMPERATURE:0.7}
%prod.openai.default-max-tokens=${DEFAULT_MAX_TOKENS:800}

# Dev/test configuration (can override with env vars)
%dev.openai/mp-rest/url=${VLLM_BASE_URL:http://localhost:8000}
%dev.openai.base-url=${VLLM_BASE_URL:http://localhost:8000}
%dev.openai.model-name=${PORO2_MODEL_NAME:LumiOpen/Llama-Poro-2-70B-Instruct}
%dev.openai.api-key=${VLLM_API_KEY:}
%dev.openai.default-temperature=${DEFAULT_TEMPERATURE:0.7}
%dev.openai.default-max-tokens=${DEFAULT_MAX_TOKENS:800}

# Test configuration
%test.openai/mp-rest/url=http://localhost:8000
%test.openai.base-url=http://localhost:8000
%test.openai.model-name=test-model
%test.openai.api-key=test-key
%test.openai.default-temperature=0.7
%test.openai.default-max-tokens=800

# Logging - all logs go to stderr
quarkus.log.console.enable=true
quarkus.log.console.stderr=true

